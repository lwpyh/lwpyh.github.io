<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jian Hu</title>

    <meta name="author" content="Jian Hu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:930px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:top">
                <p style="text-align: center;">
                  <name>Jian Hu</name>
                </p>
                <p>I am currently a final year PhD student within the <a href="http://vision.eecs.qmul.ac.uk">Computer Vision Group</a> in the School of Electronic Engineering and Computer Science at <a href="http://www.qmul.ac.uk">Queen Mary, University of London</a>, supervised by <a href="http://www.eecs.qmul.ac.uk/~sgg/">Prof. Shaogang Gong</a>.
                </p>
                <p>
                  Before joining the Computer Vision Group in QMUL, I was in <a href="http://en.sjtu.edu.cn/">Shanghai Jiao Tong University </a>, supervised by <a href="https://www.aero.sjtu.edu.cn/szdw/szml/44">Prof. Hongya Tuo</a> and  working closely with <a href="https://thinklab.sjtu.edu.cn">Prof. Junchi Yan</a>.
					</p>
					<p>	Currently, my research focuses on deep learning and computer vision, with particular emphasis on test-time training and adaptation of Multi-modal Large Language Models (MLLMs) and their applications.
                <p style="text-align:center">
                  <a href="mailto:jian.hu@qmul.ac.uk">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.co.uk/citations?user=unJmXtoAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://lwpyh.github.io/">Website</a> &nbsp;/&nbsp;
                  <a href="https://github.com/lwpyh/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:25%;max-width:25%">
                <a href="images/pic_our.jpg"><img style="width:85%;max-width:85%" alt="profile photo" src="images/pic_our.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:top">
                <h2>Research</h2>
                <p>
                  I am interested in machine learning and computer vision. My current focus is on improving MLLMs' performance in real-world scenarios with limited supervision through test-time training and adaptation. </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:top">
        <h2>News</h2>
        <p>
	  <li style="margin: 5px;"> 
	    <b>2025-04:</b> <a href="https://arxiv.org/pdf/2501.18753">INT</a> is accepted to <a href="https://2025.ijcai.org/">IJCAI</a>.
	  <li style="margin: 5px;"> 
	    <b>2025-04:</b> One paper is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">TMM</a>.
	  <li style="margin: 5px;"> 
	    <b>2025-04:</b> Give a talk in <a href="https://www.bilibili.com/video/BV1DbdHYZEbq/?spm_id_from=333.1387.homepage.video_card.click&vd_source=ffae11468d3801228f60dd8991fbe169">OpenCompass</a> on our V-STaR benchmark.
	  <li style="margin: 5px;"> 
	    <b>2025-03:</b> Released a curated <a href="https://github.com/lwpyh/Awesome-MLLM-Reasoning-Collection">multimodal reasoning MLLMs collection</a> repository.
	  <li style="margin: 5px;" >
            <b>2024-11:</b> Be awarded as a top reviewer in <a href="https://neurips.cc/">NeurIPS 2024</a>.
	  <li style="margin: 5px;" >
            <b>2024-09:</b> <a href="https://lwpyh.github.io/ProMaC/">ProMaC</a> is accepted to <a href="https://neurips.cc/">NeurIPS 2024</a>.
	  <li style="margin: 5px;" >
            <b>2024-06:</b> Start a research internship in <a href="https://research.atspotify.com/">Spotify</a>.
	  <li style="margin: 5px;" >
            <b>2024-04:</b> Give a talk in BMVA workshop: <a href="https://www.bmva.org/meetings/24-04-24-Multimodal%20Learning.html">Trustworthy Multimodal Learning with Foundation Models: Bridging the Gap between AI Research and Real World Applications</a>.
          <li style="margin: 5px;" >
            <b>2023-12:</b> <a href="https://lwpyh.github.io/GenSAM/">GenSAM</a> is accepted to <a href="https://aaai.org/aaai-conference/">AAAI 2024</a>.
          </li>
          <li style="margin: 5px;" >
            <b>2023-05:</b> <a href="https://dl.acm.org/doi/pdf/10.1145/3539618.3592079">UHPKD</a> is accepted to <a href="https://sigir.org/sigir2023/">SIGIR 2023</a>.
          </li>
        </p>
      </td>
    </tr>
  </tbody>
</table>
		  
      <tr>
      <td style="padding:20px;width:100%;vertical-align:top">
        <h2>Publications</h2>
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr class="table-row">
        <td style="padding:20px;width:25%;vertical-align:top">
          <img src="images/INT.png" alt="safs_small" width="190" height="100" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
          <a href="https://arxiv.org/pdf/2501.18753">
            <span class="papertitle">INT: Instance-Specific Negative Mining for Task-Generic Promptable Segmentation</span>
          </a>
          <br>
          <strong>Jian Hu</strong>,
	  <a href="https://zxccade.github.io/">Zixu Cheng</a>,
	  <a href="https://www.eecs.qmul.ac.uk/~sgg/">Shaogang Gong</a>
          <br>
          <em>IJCAI</em>, 2025
          <br>
          <a href="https://arxiv.org/pdf/2501.18753">arXiv</a>
	  <br>
          To adaptively reduce the influence of irrelevant (negative) prior knowledge whilst to increase the use the most plausible prior knowledge, selected by negative mining with higher contrast, in order to optimise instance-specific prompts generation.
          <br>
        </td>
      </tr>
		
        <td style="padding:20px;width:25%;vertical-align:top">
          <img src="images/CADA.png" alt="safs_small" width="200" height="80" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
          <a href="https://github.com/lwpyh/lwpyh.github.io/blob/master/CADA_TMM.pdf">
            <span class="papertitle">Class-Aware Diversified Augmentation for Open-Set Single Domain Generalization</span>
          </a>
          <br>
          <strong>Jian Hu</strong>,
          <a href="http://www.eecs.qmul.ac.uk/~sgg/">Shaogang Gong</a>,
          <a href="https://lvgd.github.io/">Weitong Cai</a>,
	  <a href="https://thinklab.sjtu.edu.cn/">Junchi Yan</a>
          <br>
          <em>IEEE Transactions on Multimedia</em>, 2025
          <br>
          <a href="https://github.com/lwpyh/lwpyh.github.io/blob/master/CADA_TMM.pdf">Paper</a>
	<br>
         Explore Single domain open-set generalization with class-aware augmentation.
          <br>
        </td>
      </tr>
		
	<tr class="table-row">
        <td style="padding:20px;width:25%;vertical-align:top">
          <img src="images/frame_ProMaC.png" alt="safs_small" width="190" height="110" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
          <a href="https://arxiv.org/abs/2408.15205.pdf">
            <span class="papertitle">Leveraging Hallucinations to Reduce Manual Prompt Dependency in Promptable Segmentation</span>
          </a>
          <br>
          <strong>Jian Hu</strong>,
          <a href="https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AH70aAW7LfVn82AZckFTh_Y7mXPPLrqDH6LMWDXLBCTbnSLe39ue9Iiza6jy5HDuReAozF5HnWECuq9xlCXrlw&user=l4Fps4EAAAAJ">Jiayi Lin</a>,
          <a href="https://thinklab.sjtu.edu.cn/">Junchi Yan</a>,
          <a href="http://www.eecs.qmul.ac.uk/~sgg/">Shaogang Gong</a>
          <br>
          <em>NeurIPS</em>, 2024
          <br>
          <a href="https://arxiv.org/abs/2408.15205.pdf">arXiv</a>
          /
          <a href="https://lwpyh.github.io/ProMaC/">website</a>
	  /
          <a href="https://github.com/lwpyh/ProMaC_code">code</a>
	  /
	  <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:AyXYB3L0fpUJ:scholar.google.com/&output=citation&scisdr=ClE6TBmEEKO-7r3MadQ:AFWwaeYAAAAAZSLKcdR6HxTknXH2JGvNUc2bZtQ&scisig=AFWwaeYAAAAAZSLKcUZSB0WWetAFTRGOQtBAweU&scisf=4&ct=citation&cd=-1&hl=zh-CN&authuser=1">bibtex</a>
          <br>
         Using hallucinations as prior knowledge to help create specific prompts for segmenting tasks, reducing the need for manual prompts.          
	  <br>
        </td>
      </tr>
		
	<tr class="table-row">
        <td style="padding:20px;width:25%;vertical-align:top">
          <img src="images/GenSAM.png" alt="safs_small" width="190" height="110" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
          <a href="https://arxiv.org/abs/2312.07374.pdf">
            <span class="papertitle">Relax Image-Specific Prompt Requirement in SAM: A Single Generic Prompt for Segmenting Camouflaged Objects</span>
          </a>
          <br>
          <strong>Jian Hu*</strong>,
          <a href="https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AH70aAW7LfVn82AZckFTh_Y7mXPPLrqDH6LMWDXLBCTbnSLe39ue9Iiza6jy5HDuReAozF5HnWECuq9xlCXrlw&user=l4Fps4EAAAAJ">Jiayi Lin*</a>,
          <a href="https://lvgd.github.io/">Weitong Cai</a>,
          <a href="http://www.eecs.qmul.ac.uk/~sgg/">Shaogang Gong</a>
          <br>
          <em>AAAI</em>, 2024
          <br>
          <a href="https://arxiv.org/abs/2312.07374.pdf">arXiv</a>
          /
          <a href="https://lwpyh.github.io/GenSAM/">website</a>
	  /
          <a href="https://github.com/jyLin8100/GenSAM">code</a>
	  /
	  <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:AyXYB3L0fpUJ:scholar.google.com/&output=citation&scisdr=ClE6TBmEEKO-7r3MadQ:AFWwaeYAAAAAZSLKcdR6HxTknXH2JGvNUc2bZtQ&scisig=AFWwaeYAAAAAZSLKcUZSB0WWetAFTRGOQtBAweU&scisf=4&ct=citation&cd=-1&hl=zh-CN&authuser=1">bibtex</a>
          <br>
         Eliminate the need for manual prompts for SAM in various challenging segmentation tasks.
          <br>
        </td>
      </tr>
	      
	<tr class="table-row">
              <td style="padding:20px;width:25%;vertical-align:top">
                <img src="images/UHPKD.png" alt="safs_small" width="180" height="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://dl.acm.org/doi/pdf/10.1145/3539618.3592079">
                  <span class="papertitle">Uncertainty-based Heterogeneous Privileged Knowledge Distillation for Recommendation System</span>
                </a>
                <br>
                Ang Li*, <strong>Jian Hu*</strong>, Ke Ding, Xiaolu Zhang, <a href="https://scholar.google.co.uk/citations?user=mCVvloEAAAAJ">Jun Zhou</a>, <a href="https://scholar.google.co.uk/citations?user=tR5ifm8AAAAJ">Yong He</a>
                <br>
                <em>SIGIR</em>, 2023
                <br>
		<a href="https://dl.acm.org/doi/pdf/10.1145/3539618.3592079">arXiv</a>
		/
                <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:Gqp1pxYwU9AJ:scholar.google.com/&output=citation&scisdr=ClE6TBmEEKO-7r26-VA:AFWwaeYAAAAAZSK84VCXmiXYxrFwuLIJqs2POW4&scisig=AFWwaeYAAAAAZSK84fYX2nrhrzVjS4jeVO4YhfY&scisf=4&ct=citation&cd=-1&hl=zh-CN&authuser=1">bibtex</a>
               <br>Proposing a novel algorithm to address heterogeneous knowledge distillation-based transfer learning in industrial recommendation systems.<br>
              </td>
            </tr>

	<tr>
              <td style="padding:20px;width:25%;vertical-align:top">
                <img src="images/GMSD.png" alt="safs_small" width="190" height="140" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://www.researchgate.net/publication/370009851_Global-Aware_Model-Free_Self-distillation_for_Recommendation_System#fullTextFileContent">
                  <span class="papertitle">Global-Aware Model-Free Self-distillation for Recommendation System</span>
                </a>
                <br>
                Ang Li*, <strong>Jian Hu*</strong>, <a href="https://scholar.google.co.uk/citations?user=pkl2pwYAAAAJ">Lu Wei</a>, Ke Ding, Xiaolu Zhang, <a href="https://scholar.google.co.uk/citations?user=mCVvloEAAAAJ">Jun Zhou</a>, <a href="https://scholar.google.co.uk/citations?user=tR5ifm8AAAAJ">Yong He</a>
                <br>
                <em>DASFAA</em>, 2023
                <br>
		<a href="https://www.researchgate.net/publication/370009851_Global-Aware_Model-Free_Self-distillation_for_Recommendation_System#fullTextFileContent">paper</a>
		/
                <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:V0EOgx7OAiQJ:scholar.google.com/&output=citation&scisdr=ClE6TBmEEKO-7r3Ehi0:AFWwaeYAAAAAZSLCni3IuvXQAXEvodU_OfeKx2o&scisig=AFWwaeYAAAAAZSLCnrPOB9EkmdZEhsIWhscktIk&scisf=4&ct=citation&cd=-1&hl=zh-CN&authuser=1">bibtex</a>
                <br>Introducing a novel algorithm called Global-aware Model-free Self-Distillation to address label noise in training data in Alipay advertising system.<br>
              </td>
            </tr>
			
      <tr>
        <td style="padding:20px;width:25%;vertical-align:top">
          <img src="images/UTEP.png" alt="safs_small" width="190" height="140" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
          <a href="https://arxiv.org/pdf/2206.01319.pdf">
            <span class="papertitle">Learning Unbiased Transferability for Domain Adaptation by Uncertainty Modeling</span>
          </a>
          <br>
          <strong>Jian Hu*</strong>,
          <a href="https://scholar.google.co.uk/citations?user=OWzLRJwAAAAJ">Haowen Zhong*</a>,
          <a>Fei Yang</a>,
          <a href="http://www.eecs.qmul.ac.uk/~sgg/">Shaogang Gong</a>,
	  <a>Guile Wu</a>,
          <a href="https://thinklab.sjtu.edu.cn/">Junchi Yan</a>
          <br>
          <em>ECCV</em>, 2022
          <br>
          <a href="https://arxiv.org/pdf/2206.01319.pdf">arXiv</a>
          /
          <a href="https://github.com/puchapu/UTEP">code</a>
	  /
	  <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:AyXYB3L0fpUJ:scholar.google.com/&output=citation&scisdr=ClE6TBmEEKO-7r3MadQ:AFWwaeYAAAAAZSLKcdR6HxTknXH2JGvNUc2bZtQ&scisig=AFWwaeYAAAAAZSLKcUZSB0WWetAFTRGOQtBAweU&scisf=4&ct=citation&cd=-1&hl=zh-CN&authuser=1">bibtex</a>
          <br>
          Delving into the transferability estimation problem in domain adaptation and propose a non-intrusive Unbiased Transferability Estimation Plug-in (UTEP) by modeling the uncertainty of a discriminator in adversarial-based DA methods to optimize unbiased transfer. 
          <br>
        </td>
      </tr>

     <tr>
              <td style="padding:20px;width:25%;vertical-align:top">
                <img src="images/ACSN.png" alt="safs_small" width="190" height="130" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://ieeexplore.ieee.org/document/9747816">
                  <span class="papertitle">Domain adaptive YOLO for one-stage cross-domain detection</span>
                </a>
                <br>
                Ang Li*, <strong>Jian Hu*</strong>, Chilin Fu, Xiaolu Zhang, <a href="https://scholar.google.co.uk/citations?user=mCVvloEAAAAJ">Jun Zhou</a>
                <br>
		<em>ICASSP</em>, 2022
                <br>
		<a href="https://ieeexplore.ieee.org/document/9747816">Paper</a>
		/
                <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:t23bCLFw88IJ:scholar.google.com/&output=citation&scisdr=ClE6TBmEEKO-7r3Om5Y:AFWwaeYAAAAAZSLIg5bSxfUVo8IoaIicN_s8Ipo&scisig=AFWwaeYAAAAAZSLIgzXBXZun4XZyoyctiulROnI&scisf=4&ct=citation&cd=-1&hl=zh-CN&authuser=1">bibtex</a>
                <br>
		A novel Attribute-Conditioned Face Swapping Network (AFSNet) to preserve attributes and handle low resolution images.
              </td>
            </tr>
     <tr>
              <td style="padding:20px;width:25%;vertical-align:top">
                <img src="images/DA_YOLO.png" alt="safs_small" width="190" height="130" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2106.13939">
                  <span class="papertitle">Attribute-Conditioned Face Swapping Network for Low-Resolution Images</span>
                </a>
                <br>
                Shizhao Zhang, Hongya Tuo, <a href="https://scholar.google.co.uk/citations?user=tR5ifm8AAAAJ">Zhongliang Jing</a>, <strong>Jian Hu</strong>
                <br>
                <em>ACML</em>, 2021
                <br>
		<a href="https://arxiv.org/abs/2106.13939">arXiv</a>
		/
                <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:t23bCLFw88IJ:scholar.google.com/&output=citation&scisdr=ClE6TBmEEKO-7r3Om5Y:AFWwaeYAAAAAZSLIg5bSxfUVo8IoaIicN_s8Ipo&scisig=AFWwaeYAAAAAZSLIgzXBXZun4XZyoyctiulROnI&scisf=4&ct=citation&cd=-1&hl=zh-CN&authuser=1">bibtex</a>
                <br>
		Improving cross-domain performance for one-stage detectors, image level features alignment is used to strictly match for local features, and loosely match for global features.
                <br>
              </td>
            </tr>

	 <tr>
        <td style="padding:20px;width:25%;vertical-align:top">
          <img src="images/DPDAN.png" alt="safs_small" width="190" height="140" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
          <a href="https://www.researchgate.net/profile/Jian-Hu-31/publication/343760785_Discriminative_Partial_Domain_Adversarial_Network/links/60c58e574585157774d23f6e/Discriminative-Partial-Domain-Adversarial-Network.pdf">
            <span class="papertitle">Discriminative Partial Domain Adversarial Network</span>
          </a>
          <br>
          <strong>Jian Hu</strong>, Hongya Tuo, Chao Wang, Lingfeng Qiao,
          <a href="https://scholar.google.co.uk/citations?user=OWzLRJwAAAAJ">Haowen Zhong</a>,
          <a href="https://thinklab.sjtu.edu.cn/">Junchi Yan</a>, <a href="https://scholar.google.co.uk/citations?user=tR5ifm8AAAAJ">Zhongliang Jing</a>, Henry Leung
          <br>
          <em>ECCV</em>, 2020
          <br>
          <a href="https://www.researchgate.net/profile/Jian-Hu-31/publication/343760785_Discriminative_Partial_Domain_Adversarial_Network/links/60c58e574585157774d23f6e/Discriminative-Partial-Domain-Adversarial-Network.pdf">arXiv</a>
          /
	  <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:TahET7h35PQJ:scholar.google.com/&output=citation&scisdr=ClE6TBmEEKO-7r3Jcuo:AFWwaeYAAAAAZSLPauoEO0N1pdwKIIVEBXm0qUQ&scisig=AFWwaeYAAAAAZSLPal0PImG37DD2GFo6j8tYmlA&scisf=4&ct=citation&cd=-1&hl=zh-CN&authuser=1">bibtex</a>
          <br>
          Addressing partial domain adaptation problem with discriminative partial domain adversarial network with theoretical analysis. 
          <br>
        </td>
      </tr>

	
        <td style="padding:20px;width:25%;vertical-align:top">
          <img src="images/USIC.png" alt="safs_small" width="190" height="140" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
          <a href="http://www.utias.utoronto.ca/wp-content/uploads/2019/07/77-Unsupervised-Satellite-Image-Classification-based-on-Partial-Transfer-Learning-1.pdf">
            <span class="papertitle">Unsupervised satellite image classification based on partial transfer learning</span>
          </a>
          <br>
          <strong>Jian Hu</strong>, Hongya Tuo, Chao Wang, <a href="https://scholar.google.co.uk/citations?user=OWzLRJwAAAAJ">Haowen Zhong</a>, Pan Han, Lingfeng Qiao,
          <a href="https://scholar.google.co.uk/citations?user=tR5ifm8AAAAJ">Zhongliang Jing</a>
          <br>
          <em>Aerospace Systems</em>, 2019
          <br>
          <a href="http://www.utias.utoronto.ca/wp-content/uploads/2019/07/77-Unsupervised-Satellite-Image-Classification-based-on-Partial-Transfer-Learning-1.pdf">arXiv</a>
          /
	  <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:OrN-JiY2cqYJ:scholar.google.com/&output=citation&scisdr=ClE6TBmEEKO-7r3kElk:AFWwaeYAAAAAZSLiClmzglQZo6oVWTZrmU8wiKc&scisig=AFWwaeYAAAAAZSLiCrRsMROzTkE1wn-wSFFXgyk&scisf=4&ct=citation&cd=-1&hl=zh-CN&authuser=1&scfhb=1">bibtex</a>
          <br>
          Focusing on how to achieve high accuracy on unsupervised satellite image classification. 
          <br>
        </td>
      </tr>
		  
	 <tr>
        <td style="padding:20px;width:25%;vertical-align:top">
          <img src="images/MWPDA.png" alt="safs_small" width="190" height="140" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
          <a href="https://bmvc2019.org/wp-content/uploads/papers/0406-paper.pdf">
            <span class="papertitle">Multi-Weight Partial Domain Adaptation</span>
          </a>
          <br>
          <strong>Jian Hu</strong>, Hongya Tuo, Chao Wang, Lingfeng Qiao,
          <a href="https://scholar.google.co.uk/citations?user=OWzLRJwAAAAJ">Haowen Zhong</a>,
          <a href="https://scholar.google.co.uk/citations?user=tR5ifm8AAAAJ">Zhongliang Jing</a>
          <br>
          <em>BMVC</em>, 2019 <font color="red">(spotlight)</font>
          <br>
          <a href="https://bmvc2019.org/wp-content/uploads/papers/0406-paper.pdf">arXiv</a>
          /
	  <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:6nScSZoj3ZcJ:scholar.google.com/&output=citation&scisdr=ClE6TBmEEKO-7r3SfYE:AFWwaeYAAAAAZSLUZYGXVUdvHzEPvzBUIwPZtdw&scisig=AFWwaeYAAAAAZSLUZasLi9pKkABAJo0JUSCNb2M&scisf=4&ct=citation&cd=-1&hl=zh-CN&authuser=1">bibtex</a>
          <br>
          Focusing on how to transfer knowledge from massive labelled dataset to unlabelled miniature one. 
          <br>
        </td>
      </tr>
	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="0"><tbody>
            <tr>
              <td>
                <h2>Preprint</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="0"><tbody>
	<tr>
        <td style="padding:20px;width:25%;vertical-align:top">
          <img src="images/vismap.png" alt="safs_small" width="190" height="90" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
          <a href="https://arxiv.org/abs/2504.15921">
            <span class="papertitle">ViSMaP: Unsupervised Hour-long Video Summarisation by Meta-Prompting
</span>
          </a>
          <br>
          <strong>Jian Hu</strong>,
	  <a href="https://scholar.google.co.uk/citations?user=lW6FiiYAAAAJ&hl=en">Dimitrios Korkinof</a>,
	  <a href="https://www.eecs.qmul.ac.uk/~sgg/">Shaogang Gong</a>,
	  <a href="https://scholar.google.co.uk/citations?user=wN6wcHYAAAAJ&hl=en">Mariano Beguerisse-Diaz</a>
          <br>
          <em>Under review</em> 
          <br>
          <a href="https://arxiv.org/abs/2504.15921">arXiv</a>
          <br>
          A system to summarise hour long videos with no-supervision. 
          <br>
        </td>
      </tr>
	<tr>
        <td style="padding:20px;width:25%;vertical-align:top">
          <img src="images/COS.png" alt="safs_small" width="190" height="70" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
          <a href="https://arxiv.org/pdf/2502.06428">
            <span class="papertitle">CoS: Chain-of-Shot Prompting for Long Video Understanding</span>
          </a>
          <br>
          <strong>Jian Hu</strong>,
	  <a href="https://zxccade.github.io/">Zixu Cheng</a>,
	  <a href="https://chenyangsi.top/">Chenyang Si</a>, 
	  <a href="https://weivision.github.io/">Wei Li</a>, 
	  <a href="https://www.eecs.qmul.ac.uk/~sgg/">Shaogang Gong</a>
          <br>
          <em>Under review</em> 
          <br>
          <a href="https://arxiv.org/pdf/2502.06428">arXiv</a>
          /
	  <a href="https://lwpyh.github.io/CoS/">website</a>
	  /
	  <a href="https://github.com/lwpyh/CoS_codes">code</a>
          <br>
          To frame shot selection as test-time visual prompt optimisation. 
          <br>
        </td>
      </tr>

	<tr>
        <td style="padding:20px;width:25%;vertical-align:top">
          <img src="images/motivation_v2.png" alt="safs_small" width="190" height="140" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
          <a href="https://arxiv.org/pdf/2503.11495">
            <span class="papertitle">V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning</span>
          </a>
          <br>
          <a href="https://zxccade.github.io/">Zixu Cheng</a>,
	  <strong>Jian Hu</strong>(Corresponding), 
          <a href="https://sites.google.com/view/ziquanliu">Ziquan Liu</a>,
          <a href="https://chenyangsi.top/">Chenyang Si</a>, 
	  <a href="https://weivision.github.io/">Wei Li</a>, 
	  <a href="https://www.eecs.qmul.ac.uk/~sgg/">Shaogang Gong</a>
          <br>
          <em>Under review</em>
          <br>
          <a href="https://arxiv.org/pdf/2503.11495">arXiv</a>
          /
	  <a href="https://v-star-bench.github.io/">website</a>
	  /
	  <a href="https://github.com/V-STaR-Bench/V-STaR">code</a>
	  <br>
          To decompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR) task that simultaneously evaluates what objects are present, when events occur, and where they are located while capturing the underlying Chain-of-thought (CoT) logic.
          <br>
        </td>
      </tr>

          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Service</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:top"><img src="images/cvf.jpg"></td>
              <td width="75%" valign="center">
                Reviewer for CVPR, ICCV, ECCV, TPAMI, IJCV, ICML, ICLR, NeurlPS (top reviewer'24), AAAI, TMLR, ACMMM (outstanding reviewer'24), PKDD, AISTATS, ToMM
              </td>
            </tr>
            <tr>
              <td style="padding:18px;width:20%;vertical-align:top">
                <img src="images/logo_QMUL.png" alt="cs188">
              </td>
              <td width="75%" valign="center">
                <a href="http://www.eecs.qmul.ac.uk/~sgg/_ECS795P_/">Student Demonstrator, ECS795P Deep Learning and Computer Vision 2022-24</a>
                <br>
		<a href="https://qmplus.qmul.ac.uk/course/view.php?id=15774">Student Demonstrator, ECS607U Data Mining 2023-24</a>
              </td>
            </tr>
        
            
           
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Special thank you to the <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
